{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event-Based Eye Tracking Tutorial\n",
    "\n",
    "This notebook introduces event-based eye tracking and shows how to:\n",
    "1. Load and visualize event camera data from the **Ini-30** and **3ET** datasets\n",
    "2. Understand the eye tracking problem formulation\n",
    "3. Understand Speck hardware constraints\n",
    "\n",
    "**Reference resources** (for learning, not copying!):\n",
    "- [RETINA repository](https://github.com/pbonazzi/retina) - Reference SNN implementation\n",
    "- [3ET repository](https://github.com/qinche106/cb-convlstm-eyetracking) - Reference ConvLSTM implementation\n",
    "- [Sinabs tutorials](https://sinabs.readthedocs.io/v3.1.1/tutorials/tutorials.html) - SNN training\n",
    "- [Speck tutorials](https://sinabs.readthedocs.io/v3.1.1/speck/tutorials.html) - Hardware deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# You can also set paths directly here\n",
    "INI30_PATH = os.getenv(\"INI30_DATA_PATH\", \"./retina/evs_ini30\")\n",
    "THREET_PATH = os.getenv(\"THREET_DATA_PATH\", \"./3et_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Event-Based Vision?\n",
    "\n",
    "Traditional cameras capture frames at fixed intervals (e.g., 30 FPS). Event cameras (Dynamic Vision Sensors - DVS) work differently:\n",
    "\n",
    "- Each pixel **independently** detects changes in brightness\n",
    "- When brightness changes by a threshold, the pixel emits an **event**\n",
    "- Events are **asynchronous** with microsecond resolution\n",
    "\n",
    "An event is a tuple: `(t, x, y, p)`\n",
    "- `t`: timestamp (microseconds)\n",
    "- `x, y`: pixel coordinates\n",
    "- `p`: polarity (+1 for brightness increase, -1 for decrease)\n",
    "\n",
    "**Advantages for eye tracking:**\n",
    "- High temporal resolution (~1 MHz vs 30-120 Hz)\n",
    "- Low latency\n",
    "- Low power consumption\n",
    "- No motion blur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Ini-30 Dataset\n",
    "\n",
    "The Ini-30 dataset contains:\n",
    "- 30 subjects with real DVS eye recordings\n",
    "- Events stored in `.aedat4` format\n",
    "- Labels: pupil center coordinates at ~50 Hz\n",
    "\n",
    "Download from: https://zenodo.org/records/11203260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check dataset structure\n",
    "ini30_path = Path(INI30_PATH)\n",
    "print(f\"Dataset path: {ini30_path}\")\n",
    "print(f\"\\nSubjects found: {sorted([d.name for d in ini30_path.iterdir() if d.is_dir()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the master annotations file\n",
    "silver_csv = ini30_path / \"silver.csv\"\n",
    "labels_df = pd.read_csv(silver_csv, delimiter='\\t')\n",
    "print(f\"Total samples: {len(labels_df)}\")\n",
    "print(f\"\\nColumns: {labels_df.columns.tolist()}\")\n",
    "print(f\"\\nSample entries:\")\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load events from one subject using dv-processing\n",
    "try:\n",
    "    import dv_processing as dv\n",
    "    \n",
    "    subject = \"ID_001\"\n",
    "    aedat_path = ini30_path / subject / \"events.aedat4\"\n",
    "    \n",
    "    # Open the aedat4 file\n",
    "    reader = dv.io.MonoCameraRecording(str(aedat_path))\n",
    "    \n",
    "    print(f\"Camera name: {reader.getCameraName()}\")\n",
    "    print(f\"Resolution: {reader.getEventResolution()}\")\n",
    "    \n",
    "    # Read first batch of events\n",
    "    events = reader.getNextEventBatch()\n",
    "    print(f\"\\nFirst batch: {len(events)} events\")\n",
    "    print(f\"Time range: {events.timestamps()[0]} - {events.timestamps()[-1]} us\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"dv-processing not installed. Install with: pip install dv-processing\")\n",
    "    print(\"Alternative: use the code from the RETINA repository\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize events as an accumulated frame\n",
    "try:\n",
    "    import dv_processing as dv\n",
    "    \n",
    "    # Read more events for visualization\n",
    "    reader = dv.io.MonoCameraRecording(str(aedat_path))\n",
    "    \n",
    "    # Accumulate events over 50ms\n",
    "    accumulator = dv.Accumulator(reader.getEventResolution())\n",
    "    accumulator.setMinPotential(0.0)\n",
    "    accumulator.setMaxPotential(1.0)\n",
    "    accumulator.setNeutralPotential(0.5)\n",
    "    accumulator.setEventContribution(0.15)\n",
    "    accumulator.setDecayFunction(dv.Accumulator.Decay.LINEAR)\n",
    "    accumulator.setDecayParam(1e-6)\n",
    "    \n",
    "    # Read events for ~50ms\n",
    "    all_events = dv.EventStore()\n",
    "    start_time = None\n",
    "    while reader.isRunning():\n",
    "        events = reader.getNextEventBatch()\n",
    "        if events is None:\n",
    "            break\n",
    "        all_events.add(events)\n",
    "        if start_time is None:\n",
    "            start_time = events.timestamps()[0]\n",
    "        if events.timestamps()[-1] - start_time > 50000:  # 50ms\n",
    "            break\n",
    "    \n",
    "    accumulator.accept(all_events)\n",
    "    frame = accumulator.generateFrame()\n",
    "    \n",
    "    # Load corresponding label\n",
    "    annotations = pd.read_csv(ini30_path / subject / \"annotations.csv\")\n",
    "    first_label = annotations.iloc[0]\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accumulated frame\n",
    "    axes[0].imshow(frame.image, cmap='gray')\n",
    "    axes[0].scatter([first_label['center_x']], [first_label['center_y']], \n",
    "                    c='red', s=100, marker='x', linewidths=2, label='Pupil center')\n",
    "    axes[0].set_title(f'Accumulated Events (50ms) - {subject}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Event scatter plot\n",
    "    coords = all_events.coordinates()\n",
    "    pols = all_events.polarities()\n",
    "    sample_idx = np.random.choice(len(coords), min(5000, len(coords)), replace=False)\n",
    "    \n",
    "    axes[1].scatter(coords[sample_idx, 0], coords[sample_idx, 1], \n",
    "                    c=pols[sample_idx], cmap='coolwarm', s=1, alpha=0.5)\n",
    "    axes[1].scatter([first_label['center_x']], [first_label['center_y']], \n",
    "                    c='green', s=100, marker='x', linewidths=2)\n",
    "    axes[1].set_xlim(0, 640)\n",
    "    axes[1].set_ylim(480, 0)\n",
    "    axes[1].set_title('Event Scatter (sampled, red=ON, blue=OFF)')\n",
    "    axes[1].set_xlabel('x')\n",
    "    axes[1].set_ylabel('y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nLabel coordinates: ({first_label['center_x']:.1f}, {first_label['center_y']:.1f})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading 3ET Synthetic Dataset\n",
    "\n",
    "The 3ET dataset contains:\n",
    "- 22 subjects with synthetic event-based eye recordings\n",
    "- Events stored in `.h5` format (pre-processed event frames)\n",
    "- Labels: pupil center (x, y) coordinates\n",
    "\n",
    "**Download options:**\n",
    "- [H5 event frames (recommended)](https://drive.google.com/drive/folders/16qH_wv_oVNysJARtHIUrIXbHjOygfq_i)\n",
    "- [AEDAT raw + video files](https://drive.google.com/drive/folders/1HeOS5YBLruzHjwMKyBQfVTc_mJbsy_R1)\n",
    "\n",
    "**Repository:** https://github.com/qinche106/cb-convlstm-eyetracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "threet_path = Path(THREET_PATH)\n",
    "print(f\"3ET path: {threet_path}\")\n",
    "\n",
    "# Check if data exists\n",
    "if threet_path.exists():\n",
    "    # 3ET dataset structure varies - check what's available\n",
    "    h5_files = list(threet_path.rglob(\"*.h5\"))\n",
    "    if h5_files:\n",
    "        print(f\"Found {len(h5_files)} H5 files\")\n",
    "        print(f\"Example files: {[f.name for f in h5_files[:5]]}\")\n",
    "    else:\n",
    "        print(\"No H5 files found. Download the dataset from Google Drive.\")\n",
    "else:\n",
    "    print(f\"Path {threet_path} does not exist. Set THREET_DATA_PATH in .env\")\n",
    "    print(\"Download from: https://drive.google.com/drive/folders/16qH_wv_oVNysJARtHIUrIXbHjOygfq_i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample from 3ET (if available)\n",
    "# Note: 3ET H5 files contain pre-processed event frames, not raw events\n",
    "try:\n",
    "    h5_files = list(threet_path.rglob(\"*.h5\"))\n",
    "    if h5_files:\n",
    "        h5_file = h5_files[0]\n",
    "        print(f\"Loading: {h5_file.name}\")\n",
    "        \n",
    "        with h5py.File(h5_file, 'r') as f:\n",
    "            print(f\"\\nH5 file structure:\")\n",
    "            for key in f.keys():\n",
    "                print(f\"  {key}: shape={f[key].shape}, dtype={f[key].dtype}\")\n",
    "            \n",
    "            # Try to load and visualize\n",
    "            if 'vector' in f.keys():\n",
    "                data = f['vector'][:]\n",
    "                print(f\"\\nData shape: {data.shape}\")\n",
    "                \n",
    "                # Visualize a few frames\n",
    "                if len(data.shape) >= 2:\n",
    "                    fig, axes = plt.subplots(1, min(4, len(data)), figsize=(16, 4))\n",
    "                    if len(data) == 1:\n",
    "                        axes = [axes]\n",
    "                    for i, ax in enumerate(axes):\n",
    "                        frame_data = data[i].squeeze() if len(data[i].shape) > 2 else data[i]\n",
    "                        ax.imshow(frame_data, cmap='gray')\n",
    "                        ax.set_title(f'Frame {i}')\n",
    "                        ax.axis('off')\n",
    "                    plt.suptitle(f'3ET Event Frames from {h5_file.name}')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "    else:\n",
    "        print(\"No H5 files found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load 3ET data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Event Representations\n",
    "\n",
    "Raw events need to be converted into a format suitable for neural networks. Common representations:\n",
    "\n",
    "### 4.1 Event Frames (Histogram)\n",
    "Accumulate events over a time window into a 2D histogram.\n",
    "\n",
    "```\n",
    "frame[y, x] = count of events at (x, y) in time window\n",
    "```\n",
    "\n",
    "### 4.2 Voxel Grid (Temporal Bins)\n",
    "Divide time into bins and create a 3D tensor.\n",
    "\n",
    "```\n",
    "voxel[t_bin, y, x] = weighted sum of events in time bin t_bin at (x, y)\n",
    "```\n",
    "\n",
    "### 4.3 Time Surfaces\n",
    "Store the most recent timestamp at each pixel.\n",
    "\n",
    "```\n",
    "surface[y, x] = most recent event timestamp at (x, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_to_frame(events, height=480, width=640, separate_polarity=True):\n",
    "    \"\"\"\n",
    "    Convert events to a frame representation.\n",
    "    \n",
    "    Args:\n",
    "        events: structured array with fields 't', 'x', 'y', 'p'\n",
    "        height, width: sensor dimensions\n",
    "        separate_polarity: if True, create 2 channels (ON/OFF)\n",
    "    \n",
    "    Returns:\n",
    "        frame: (C, H, W) tensor where C=2 if separate_polarity else C=1\n",
    "    \"\"\"\n",
    "    if separate_polarity:\n",
    "        frame = np.zeros((2, height, width), dtype=np.float32)\n",
    "        \n",
    "        # ON events (p=1)\n",
    "        on_mask = events['p'] == 1\n",
    "        np.add.at(frame[0], (events['y'][on_mask], events['x'][on_mask]), 1)\n",
    "        \n",
    "        # OFF events (p=0 or p=-1)\n",
    "        off_mask = ~on_mask\n",
    "        np.add.at(frame[1], (events['y'][off_mask], events['x'][off_mask]), 1)\n",
    "    else:\n",
    "        frame = np.zeros((1, height, width), dtype=np.float32)\n",
    "        np.add.at(frame[0], (events['y'], events['x']), 1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "\n",
    "def events_to_voxel_grid(events, num_bins=5, height=480, width=640):\n",
    "    \"\"\"\n",
    "    Convert events to a voxel grid representation.\n",
    "    \n",
    "    Args:\n",
    "        events: structured array with fields 't', 'x', 'y', 'p'\n",
    "        num_bins: number of temporal bins\n",
    "        height, width: sensor dimensions\n",
    "    \n",
    "    Returns:\n",
    "        voxel: (num_bins, H, W) tensor\n",
    "    \"\"\"\n",
    "    voxel = np.zeros((num_bins, height, width), dtype=np.float32)\n",
    "    \n",
    "    t = events['t'].astype(np.float32)\n",
    "    t_normalized = (t - t.min()) / (t.max() - t.min() + 1e-6)  # Normalize to [0, 1]\n",
    "    t_bins = (t_normalized * (num_bins - 1)).astype(np.int32)\n",
    "    t_bins = np.clip(t_bins, 0, num_bins - 1)\n",
    "    \n",
    "    # Polarity as weight\n",
    "    p = events['p'].astype(np.float32) * 2 - 1  # Convert to -1/+1\n",
    "    \n",
    "    for i in range(len(events)):\n",
    "        voxel[t_bins[i], events['y'][i], events['x'][i]] += p[i]\n",
    "    \n",
    "    return voxel\n",
    "\n",
    "\n",
    "print(\"Event representation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different representations\n",
    "try:\n",
    "    import dv_processing as dv\n",
    "    \n",
    "    # Load some events\n",
    "    reader = dv.io.MonoCameraRecording(str(ini30_path / \"ID_001\" / \"events.aedat4\"))\n",
    "    all_events = dv.EventStore()\n",
    "    start_time = None\n",
    "    while reader.isRunning():\n",
    "        events = reader.getNextEventBatch()\n",
    "        if events is None:\n",
    "            break\n",
    "        all_events.add(events)\n",
    "        if start_time is None:\n",
    "            start_time = events.timestamps()[0]\n",
    "        if events.timestamps()[-1] - start_time > 30000:  # 30ms\n",
    "            break\n",
    "    \n",
    "    # Convert to numpy structured array\n",
    "    coords = all_events.coordinates()\n",
    "    times = all_events.timestamps()\n",
    "    pols = all_events.polarities()\n",
    "    \n",
    "    events_array = np.zeros(len(times), dtype=[('t', '<i8'), ('x', '<i4'), ('y', '<i4'), ('p', '<i4')])\n",
    "    events_array['t'] = times\n",
    "    events_array['x'] = coords[:, 0]\n",
    "    events_array['y'] = coords[:, 1]\n",
    "    events_array['p'] = pols\n",
    "    \n",
    "    # Create representations\n",
    "    frame = events_to_frame(events_array, separate_polarity=True)\n",
    "    voxel = events_to_voxel_grid(events_array, num_bins=5)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Frame representation\n",
    "    axes[0, 0].imshow(frame[0], cmap='Reds')\n",
    "    axes[0, 0].set_title('Event Frame - ON channel')\n",
    "    \n",
    "    axes[0, 1].imshow(frame[1], cmap='Blues')\n",
    "    axes[0, 1].set_title('Event Frame - OFF channel')\n",
    "    \n",
    "    axes[0, 2].imshow(frame[0] - frame[1], cmap='coolwarm')\n",
    "    axes[0, 2].set_title('Event Frame - Combined')\n",
    "    \n",
    "    # Voxel grid (show 3 time bins)\n",
    "    for i, bin_idx in enumerate([0, 2, 4]):\n",
    "        axes[1, i].imshow(voxel[bin_idx], cmap='coolwarm', vmin=-5, vmax=5)\n",
    "        axes[1, i].set_title(f'Voxel Grid - Time bin {bin_idx}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Frame shape: {frame.shape}\")\n",
    "    print(f\"Voxel shape: {voxel.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Speck Hardware Constraints\n",
    "\n",
    "To deploy on Speck, your model must satisfy these constraints:\n",
    "\n",
    "| Constraint | Requirement |\n",
    "|------------|-------------|\n",
    "| **Neuron type** | IAF (Integrate-and-Fire) only |\n",
    "| **Activation** | Spiking neurons (no ReLU/Sigmoid) |\n",
    "| **Pooling** | Sum pooling only (no Max/Avg) |\n",
    "| **Weights** | 8-bit quantized |\n",
    "| **Neuron states** | 16-bit |\n",
    "| **Max layers** | 9 convolutional layers |\n",
    "| **Max input** | 128x128 pixels |\n",
    "| **Max channels** | 1024 per layer |\n",
    "\n",
    "### Memory Constraints\n",
    "\n",
    "Each layer must fit within the chip's memory:\n",
    "\n",
    "**Kernel Memory (KMT):**\n",
    "$$KMT = c \\times 2^{\\lceil\\log_2(k_x \\times k_y)\\rceil + \\lceil\\log_2(f)\\rceil}$$\n",
    "\n",
    "**Neuron Memory (NM):**\n",
    "$$NM = f \\times f_x \\times f_y$$\n",
    "\n",
    "Where:\n",
    "- $c$ = input channels\n",
    "- $f$ = output channels (filters)\n",
    "- $k_x, k_y$ = kernel size\n",
    "- $f_x, f_y$ = output feature map size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check if a model is Speck-compatible using sinabs\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import sinabs\n",
    "    import sinabs.layers as sl\n",
    "    from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "    \n",
    "    # Define a simple Speck-compatible SNN\n",
    "    class SimpleSNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.seq = nn.Sequential(\n",
    "                # Conv layer 1\n",
    "                nn.Conv2d(2, 16, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                sl.IAFSqueeze(batch_size=1),\n",
    "                \n",
    "                # Conv layer 2  \n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                sl.IAFSqueeze(batch_size=1),\n",
    "                \n",
    "                # Conv layer 3\n",
    "                nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                sl.IAFSqueeze(batch_size=1),\n",
    "                \n",
    "                # Output\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(32 * 8 * 8, 2, bias=False),  # 2 outputs: x, y\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.seq(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = SimpleSNN()\n",
    "    print(\"Model architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Try to convert for Speck\n",
    "    input_shape = (2, 64, 64)  # 2 channels, 64x64 input\n",
    "    \n",
    "    dynapcnn_net = DynapcnnNetwork(\n",
    "        snn=model.seq,\n",
    "        input_shape=input_shape,\n",
    "        discretize=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel converted successfully!\")\n",
    "    print(f\"Number of DynapCNN layers: {len(dynapcnn_net.sequence)}\")\n",
    "    \n",
    "    # Check memory mapping\n",
    "    try:\n",
    "        config = dynapcnn_net.make_config(device=\"speck2fmodule\")\n",
    "        print(\"\\nMemory validation: PASSED - Model fits on Speck!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nMemory validation: FAILED - {e}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"sinabs not installed or import error: {e}\")\n",
    "    print(\"Install with: pip install sinabs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Eye Tracking Problem Formulation\n",
    "\n",
    "**Input**: Event stream from DVS camera looking at an eye\n",
    "\n",
    "**Output**: Pupil center coordinates $(x, y)$\n",
    "\n",
    "**Approach**: \n",
    "1. Accumulate events into frames/voxels (temporal windowing)\n",
    "2. Feed through SNN\n",
    "3. Predict coordinates (regression)\n",
    "\n",
    "**Evaluation Metric**: Euclidean distance in pixels\n",
    "\n",
    "$$\\text{Error} = \\sqrt{(x_{pred} - x_{true})^2 + (y_{pred} - y_{true})^2}$$\n",
    "\n",
    "### Typical Pipeline\n",
    "\n",
    "```\n",
    "DVS Events -> Temporal Windowing -> Event Frames -> SNN -> (x, y) prediction\n",
    "                                      |\n",
    "                              Downscale to 64x64\n",
    "                              (Speck compatible)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_error(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance error.\n",
    "    \n",
    "    Args:\n",
    "        predictions: (N, 2) array of (x, y) coordinates\n",
    "        targets: (N, 2) array of (x, y) coordinates\n",
    "    \n",
    "    Returns:\n",
    "        Mean Euclidean distance in pixels\n",
    "    \"\"\"\n",
    "    return np.mean(np.sqrt(np.sum((predictions - targets) ** 2, axis=1)))\n",
    "\n",
    "\n",
    "# Example\n",
    "predictions = np.array([[30.5, 25.2], [31.0, 24.8], [29.8, 25.5]])\n",
    "targets = np.array([[32.0, 25.0], [32.0, 25.0], [32.0, 25.0]])\n",
    "\n",
    "error = euclidean_error(predictions, targets)\n",
    "print(f\"Example predictions: {predictions}\")\n",
    "print(f\"Ground truth: {targets[0]}\")\n",
    "print(f\"Mean Euclidean error: {error:.2f} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Your Challenge\n",
    "\n",
    "Now that you understand the data and constraints, it's time to build something **new**!\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Create a novel SNN-based eye tracking solution that:\n",
    "1. Predicts pupil coordinates with good accuracy\n",
    "2. Runs on Speck hardware (or SpecksimSimulator)\n",
    "3. **Brings something new** - don't just reproduce existing work!\n",
    "\n",
    "\n",
    "### Ideas to Explore\n",
    "\n",
    "**Architecture innovations:**\n",
    "- Attention mechanisms for SNNs\n",
    "- Skip connections / residual learning\n",
    "- Different temporal aggregation strategies\n",
    "- Efficient micro-architectures\n",
    "\n",
    "**Training strategies:**\n",
    "- Novel surrogate gradients\n",
    "- Knowledge distillation\n",
    "- Self-supervised pre-training\n",
    "- Data augmentation for events\n",
    "\n",
    "**Input representations:**\n",
    "- Learned event encodings\n",
    "- Multi-scale processing\n",
    "- Adaptive temporal windowing\n",
    "- Hybrid representations\n",
    "\n",
    "### Learning Resources\n",
    "\n",
    "- [Sinabs BPTT Tutorial](https://sinabs.readthedocs.io/v3.1.1/tutorials/bptt.html)\n",
    "- [ANN to SNN Conversion](https://sinabs.readthedocs.io/v3.1.1/tutorials/ann_to_snn_conversion.html)\n",
    "- [Speck Deployment Guide](https://sinabs.readthedocs.io/v3.1.1/speck/nmnist.html)\n",
    "- [Specksim](https://sinabs.readthedocs.io/v3.1.1/speck/specksim.html)\n",
    "\n",
    "### Key Tips\n",
    "\n",
    "1. **Validate Speck compatibility early** - don't train a model that won't fit!\n",
    "2. **Start simple, iterate fast** - get something working, then improve\n",
    "3. **Use 64x64 input** - standard for Speck deployment\n",
    "4. **IAF neurons only** - no LIF or other neuron types on Speck\n",
    "5. **Sum pooling only** - no max or average pooling\n",
    "6. **Be creative!** - the best solutions often come from unexpected directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Papers:**\n",
    "- [RETINA: Low-Power Eye Tracking with Event Camera and Spiking Hardware](https://arxiv.org/abs/2312.00425)\n",
    "- [3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network](https://ieeexplore.ieee.org/document/10389541)\n",
    "\n",
    "**Code Repositories:**\n",
    "- [RETINA](https://github.com/pbonazzi/retina) - Reference SNN implementation\n",
    "- [3ET ConvLSTM](https://github.com/qinche106/cb-convlstm-eyetracking) - Reference ConvLSTM implementation\n",
    "- [Sinabs Library](https://sinabs.readthedocs.io/v3.1.1/) - SNN framework for Speck\n",
    "- [Tonic](https://tonic.readthedocs.io/) - Event camera data loading\n",
    "\n",
    "**Datasets:**\n",
    "- [Ini-30 on Zenodo](https://zenodo.org/records/11203260) - Real DVS eye recordings\n",
    "- [3ET Synthetic Dataset](https://drive.google.com/drive/folders/16qH_wv_oVNysJARtHIUrIXbHjOygfq_i) - Synthetic event frames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
